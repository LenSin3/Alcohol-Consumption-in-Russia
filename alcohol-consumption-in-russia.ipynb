{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convinced-phrase",
   "metadata": {},
   "source": [
    "# Alcohol Consumption in Russia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-nowhere",
   "metadata": {},
   "source": [
    "![Alcoholic Beverages in Russia](images/drinks.png)\n",
    "            Source: [The Russian alcohol market: a heady cocktail](http://www.food-exhibitions.com/Market-Insights/Russia/The-Russian-alcohol-market)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-recorder",
   "metadata": {},
   "source": [
    "## Project Motivation\n",
    "\n",
    "A fictitious company owns a chain of stores across Russia that sell a variety of types of alcohol. The company recently ran a wine promotion in Saint Petersburg that was very successful. Due to the cost to the business, it isnâ€™t possible to run the promotion in all regions. The marketing team would like to target 10 other regions that have similar buying habits to Saint Petersburg where they would expect the promotion to be similarly successful and need help determining which regions they should select.\n",
    "\n",
    "![Regions in Russia](images/regions.png)\n",
    "        Source: [Outline of Russia](https://en.wikipedia.org/wiki/Outline_of_Russia)\n",
    "        \n",
    "This project aims to use machine learning algorithm to recommend, at least 10 regions with alcohol buying habits similar to Saint Petersburg. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-enforcement",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "The data used in this project is obtained from [Datacamp's Career Hub repository](https://github.com/datacamp/careerhub-data) on GitHub. It contains 7 variables as see in the description below:\n",
    "\n",
    "![Description of dataset](images/data_description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-master",
   "metadata": {},
   "source": [
    "## Analysis Plan\n",
    "\n",
    "Based on the ask of the project, the problem is best solved using an unsupervied machine learning algorithm that could best cluster regions based on wine sales in Saint Petersburg. Selection of this algorithm will be done in subsequent sections.\n",
    "\n",
    "The following steps will be followed:\n",
    "\n",
    "- Perform Exploratory Data Analysis to identify patters and draw insights from the data.\n",
    "- Select a suitable unsupervised machine learning algorithm based on problem to solve and information from the exploratory data analysis.\n",
    "- Discuss model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-nation",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "This section will explore the data to discover trends and insights. It will be done by creating plots of features against their values. The following steps will be implemented:\n",
    "\n",
    "- Read data\n",
    "- Check for data quality issues.\n",
    "- Data Visualization to observe patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alien-stuff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-10-10.0.19041-SP0\n",
      "Python 3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Numpy 1.19.2\n",
      "Matplotlib 3.3.2\n",
      "Pandas 1.1.5\n",
      "Seaborn 0.11.1\n",
      "Scipy 1.5.2\n",
      "Scikit -Learn 0.23.2\n"
     ]
    }
   ],
   "source": [
    "# import system and exploratory analysis modules\n",
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import numpy as np; print(\"Numpy\", np.__version__)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt; print(\"Matplotlib\", matplotlib.__version__)\n",
    "import pandas as pd; print(\"Pandas\", pd.__version__)\n",
    "import seaborn as sns; print(\"Seaborn\", sns.__version__)\n",
    "import scipy; print(\"Scipy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit -Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read data, check for nulls and drop duplicates\n",
    "def read_data(data_path):\n",
    "    # read data\n",
    "    print(\"Reading Alcohol Consumption in Russia dataset\\n\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    # make a copy of dataframe\n",
    "    print(\"Making a copy of the dataframe\\n\")\n",
    "    df_1 = df.copy()\n",
    "    # drop duplicates\n",
    "    df_final = df_1.drop_duplicates()\n",
    "    # extract feature names\n",
    "    df_cols = df_final.columns.tolist()\n",
    "    # empty list to hold data types, non nulss count, nulss count, percentage of nulls in a column,\\\n",
    "    # percentage of column nulls in datafram\n",
    "    data_types = []\n",
    "    non_nulls = []\n",
    "    nulls = []\n",
    "    null_column_percent = []\n",
    "    null_df_percent = []\n",
    "    \n",
    "    # loop through columns and capture the variables above\n",
    "    print(\"Extracting count and percentages of nulls and non nulls\")\n",
    "    for col in df_cols:\n",
    "        \n",
    "        # extract null count\n",
    "        null_count = df_final[col].isna().sum()\n",
    "        nulls.append(null_count)\n",
    "        \n",
    "        # extract non null count\n",
    "        non_null_count = len(df_final) - null_count\n",
    "        non_nulls.append(non_null_count)\n",
    "        \n",
    "        # extract % of null in column\n",
    "        col_null_perc = 100 * null_count/len(df_final)\n",
    "        null_column_percent.append(col_null_perc)\n",
    "        \n",
    "        # extract % of nulls out of total nulls in dataframe\n",
    "        df_null_perc = null_count/df_final.isna().sum().sum()\n",
    "        null_df_percent.append(df_null_perc)\n",
    "        \n",
    "        # capture data types\n",
    "        data_types.append(df_final[col].dtypes) \n",
    "    # create zipped list with column names, data_types, nulls and non nulls\n",
    "    lst_data = list(zip(df_cols, data_types, non_nulls, nulls, null_column_percent, null_df_percent))\n",
    "    # create dataframe of zipped list\n",
    "    df_zipped = pd.DataFrame(lst_data, columns = ['Feature', 'DataType', 'CountOfNonNulls', 'CountOfNulls',\\\n",
    "                                                 'PercentOfNullsIinColumn', 'PercentOfNullsInData'])\n",
    "    return df_final, df_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythonadv] *",
   "language": "python",
   "name": "conda-env-pythonadv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
